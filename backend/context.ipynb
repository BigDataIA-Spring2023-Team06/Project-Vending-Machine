{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the api key from the .env file\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the response from chatgpt\n",
    "def get_response(query):\n",
    "    response =  openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = [\n",
    "            {\"role\" : \"user\", \"content\" : query }\n",
    "        ]\n",
    "    )\n",
    "    response= response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['Give me a few data engineering projects with FastAPI,Streamlit and AWS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Give me a few data engineering projects with FastAPI,Streamlit and AWS']\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate responses from chatgpt with context\n",
    "def get_response_with_context():\n",
    "    context_counter = 0\n",
    "    response = get_response(context[context_counter])\n",
    "    context.append(response)\n",
    "    context_counter += 1\n",
    "    print(response)\n",
    "    project_title = input(\"choose a project from the above response\")\n",
    "    current_query = project_title + \"give me a starter file structures and files in each folder for this project\"\n",
    "    response= get_response(current_query)\n",
    "    context.append(response)\n",
    "    context_counter += 1\n",
    "    print(response)\n",
    "    response=get_response(project_title + context[context_counter] + \"generate starter code for these files according to the project\")\n",
    "    context.append(response)\n",
    "    context_counter += 1\n",
    "    response = get_response(project_title + context[context_counter - 1] +context[context_counter] + \"give me the remaiing code for the project\")\n",
    "    context.append(response)\n",
    "    context_counter += 1\n",
    "    response = get_response(project_title + context[context_counter - 1] +context[context_counter] + \"give me a readme file for githubfor the project\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Real-time Data Analytics Dashboard: Use FastAPI and Streamlit to build a dashboard that displays real-time data analytics for a website or app. Use AWS Kinesis or Apache Kafka to collect and process streaming data.\n",
      "\n",
      "2. Image Recognition API: Use FastAPI to create an API that provides image recognition services powered by a deep learning model. Deploy the model to AWS SageMaker and integrate with Amazon API Gateway for scalable access.\n",
      "\n",
      "3. Automated Data Pipeline: Use FastAPI to build a RESTful API that handles data ingestion, cleaning, transformation and storage with AWS Lambda, S3, and DynamoDB. Use Streamlit to create a simple UI that visualizes the data pipeline.\n",
      "\n",
      "4. Real-time Chat Application: Use FastAPI for the backend and Streamlit for the frontend to develop a real-time chat application. Deploy the application to AWS Elastic Beanstalk and integrate with Amazon SNS or SQS for scalable messaging.\n",
      "\n",
      "5. Recommendation Engine: Use FastAPI to build a backend that collects data from various sources and uses collaborative filtering algorithms to generate personalized recommendations. Use Streamlit to create a data visualization dashboard and deploy the backend to AWS Lambda and DynamoDB or Neptune for scalable storage.\n",
      "Sure, here's a possible file structure and files for the project:\n",
      "\n",
      "```\n",
      "├── main.py\n",
      "├── dashboard\n",
      "│   ├── __init__.py\n",
      "│   ├── app.py\n",
      "│   ├── components.py\n",
      "│   └── styles.css\n",
      "├── stream_processor\n",
      "│   ├── __init__.py\n",
      "│   ├── config.py\n",
      "│   └── consumer.py\n",
      "└── requirements.txt\n",
      "```\n",
      "\n",
      "The `main.py` file would contain the entry point of the application and would be responsible for starting the Stream Processor and the Dashboard. Here's an example:\n",
      "\n",
      "```python\n",
      "from dashboard.app import run_dashboard\n",
      "from stream_processor.consumer import run_consumer\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Start the Stream Processor\n",
      "    run_consumer()\n",
      "\n",
      "    # Start the Dashboard\n",
      "    run_dashboard()\n",
      "```\n",
      "\n",
      "The `dashboard` folder would contain the code for the Streamlit dashboard. The `__init__.py` is an empty file that allows importing modules from this folder. The `app.py` file would contain the main code of the Streamlit app, the `components.py` file would contain reusable components, and the `styles.css` file would contain the CSS styles applied to the app. Here's an example:\n",
      "\n",
      "```python\n",
      "# app.py\n",
      "\n",
      "import streamlit as st\n",
      "\n",
      "\n",
      "def main():\n",
      "    st.title(\"Real-time Data Analytics Dashboard\")\n",
      "\n",
      "    # Add your code here...\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "```python\n",
      "# components.py\n",
      "\n",
      "import streamlit as st\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "def render_dataframe(df: pd.DataFrame) -> None:\n",
      "    st.table(df)\n",
      "\n",
      "\n",
      "def render_line_chart(df: pd.DataFrame, x: str, y: str) -> None:\n",
      "    chart_data = df[[x, y]]\n",
      "    st.line_chart(chart_data)\n",
      "```\n",
      "\n",
      "```css\n",
      "/* styles.css */\n",
      "\n",
      "body {\n",
      "  margin: 0;\n",
      "  padding: 0;\n",
      "  font-family: sans-serif;\n",
      "}\n",
      "\n",
      ".container {\n",
      "  max-width: 1200px;\n",
      "  margin: 0 auto;\n",
      "  padding: 20px;\n",
      "}\n",
      "\n",
      "h1 {\n",
      "  text-align: center;\n",
      "  margin: 20px 0;\n",
      "}\n",
      "\n",
      "table {\n",
      "  border-collapse: collapse;\n",
      "  border-spacing: 0;\n",
      "  width: 100%;\n",
      "  border: 1px solid #ddd;\n",
      "}\n",
      "\n",
      "table th,\n",
      "table td {\n",
      "  text-align: left;\n",
      "  padding: 8px;\n",
      "}\n",
      "\n",
      "table th {\n",
      "  background-color: #f2f2f2;\n",
      "  border-bottom: 1px solid #ddd;\n",
      "}\n",
      "\n",
      "table td {\n",
      "  border-bottom: 1px solid #ddd;\n",
      "}\n",
      "\n",
      ".chart {\n",
      "  width: 100%;\n",
      "  height: 300px;\n",
      "}\n",
      "```\n",
      "\n",
      "The `stream_processor` folder would contain the code for the data stream processor. The `__init__.py` file is an empty file that allows importing modules from this folder, the `config.py` file would contain configuration settings (e.g., AWS Kinesis or Apache Kafka settings), and the `consumer.py` would contain the code that consumes the streaming data and stores it in a database for the dashboard to access. Here's an example:\n",
      "\n",
      "```python\n",
      "# config.py\n",
      "\n",
      "KINESIS_CONFIG = {\n",
      "    \"stream_name\": \"my-stream\",\n",
      "    \"region_name\": \"us-west-2\",\n",
      "}\n",
      "```\n",
      "\n",
      "```python\n",
      "# consumer.py\n",
      "\n",
      "from stream_processor.config import KINESIS_CONFIG\n",
      "\n",
      "\n",
      "def run_consumer() -> None:\n",
      "    # Add your code here...\n",
      "```\n",
      "\n",
      "Finally, the `requirements.txt` file would contain the list of Python packages required for the project:\n",
      "\n",
      "```\n",
      "fastapi\n",
      "streamlit\n",
      "pandas\n",
      "boto3  # for AWS Kinesis\n",
      "kafka-python  # for Apache Kafka\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Give me a few data engineering projects with FastAPI,Streamlit and AWS',\n",
       " '1. Real-time Data Analytics Dashboard: Use FastAPI and Streamlit to build a dashboard that displays real-time data analytics for a website or app. Use AWS Kinesis or Apache Kafka to collect and process streaming data.\\n\\n2. Image Recognition API: Use FastAPI to create an API that provides image recognition services powered by a deep learning model. Deploy the model to AWS SageMaker and integrate with Amazon API Gateway for scalable access.\\n\\n3. Automated Data Pipeline: Use FastAPI to build a RESTful API that handles data ingestion, cleaning, transformation and storage with AWS Lambda, S3, and DynamoDB. Use Streamlit to create a simple UI that visualizes the data pipeline.\\n\\n4. Real-time Chat Application: Use FastAPI for the backend and Streamlit for the frontend to develop a real-time chat application. Deploy the application to AWS Elastic Beanstalk and integrate with Amazon SNS or SQS for scalable messaging.\\n\\n5. Recommendation Engine: Use FastAPI to build a backend that collects data from various sources and uses collaborative filtering algorithms to generate personalized recommendations. Use Streamlit to create a data visualization dashboard and deploy the backend to AWS Lambda and DynamoDB or Neptune for scalable storage.',\n",
       " 'Sure, here\\'s a possible file structure and files for the project:\\n\\n```\\n├── main.py\\n├── dashboard\\n│   ├── __init__.py\\n│   ├── app.py\\n│   ├── components.py\\n│   └── styles.css\\n├── stream_processor\\n│   ├── __init__.py\\n│   ├── config.py\\n│   └── consumer.py\\n└── requirements.txt\\n```\\n\\nThe `main.py` file would contain the entry point of the application and would be responsible for starting the Stream Processor and the Dashboard. Here\\'s an example:\\n\\n```python\\nfrom dashboard.app import run_dashboard\\nfrom stream_processor.consumer import run_consumer\\n\\n\\nif __name__ == \"__main__\":\\n    # Start the Stream Processor\\n    run_consumer()\\n\\n    # Start the Dashboard\\n    run_dashboard()\\n```\\n\\nThe `dashboard` folder would contain the code for the Streamlit dashboard. The `__init__.py` is an empty file that allows importing modules from this folder. The `app.py` file would contain the main code of the Streamlit app, the `components.py` file would contain reusable components, and the `styles.css` file would contain the CSS styles applied to the app. Here\\'s an example:\\n\\n```python\\n# app.py\\n\\nimport streamlit as st\\n\\n\\ndef main():\\n    st.title(\"Real-time Data Analytics Dashboard\")\\n\\n    # Add your code here...\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n```python\\n# components.py\\n\\nimport streamlit as st\\nimport pandas as pd\\n\\n\\ndef render_dataframe(df: pd.DataFrame) -> None:\\n    st.table(df)\\n\\n\\ndef render_line_chart(df: pd.DataFrame, x: str, y: str) -> None:\\n    chart_data = df[[x, y]]\\n    st.line_chart(chart_data)\\n```\\n\\n```css\\n/* styles.css */\\n\\nbody {\\n  margin: 0;\\n  padding: 0;\\n  font-family: sans-serif;\\n}\\n\\n.container {\\n  max-width: 1200px;\\n  margin: 0 auto;\\n  padding: 20px;\\n}\\n\\nh1 {\\n  text-align: center;\\n  margin: 20px 0;\\n}\\n\\ntable {\\n  border-collapse: collapse;\\n  border-spacing: 0;\\n  width: 100%;\\n  border: 1px solid #ddd;\\n}\\n\\ntable th,\\ntable td {\\n  text-align: left;\\n  padding: 8px;\\n}\\n\\ntable th {\\n  background-color: #f2f2f2;\\n  border-bottom: 1px solid #ddd;\\n}\\n\\ntable td {\\n  border-bottom: 1px solid #ddd;\\n}\\n\\n.chart {\\n  width: 100%;\\n  height: 300px;\\n}\\n```\\n\\nThe `stream_processor` folder would contain the code for the data stream processor. The `__init__.py` file is an empty file that allows importing modules from this folder, the `config.py` file would contain configuration settings (e.g., AWS Kinesis or Apache Kafka settings), and the `consumer.py` would contain the code that consumes the streaming data and stores it in a database for the dashboard to access. Here\\'s an example:\\n\\n```python\\n# config.py\\n\\nKINESIS_CONFIG = {\\n    \"stream_name\": \"my-stream\",\\n    \"region_name\": \"us-west-2\",\\n}\\n```\\n\\n```python\\n# consumer.py\\n\\nfrom stream_processor.config import KINESIS_CONFIG\\n\\n\\ndef run_consumer() -> None:\\n    # Add your code here...\\n```\\n\\nFinally, the `requirements.txt` file would contain the list of Python packages required for the project:\\n\\n```\\nfastapi\\nstreamlit\\npandas\\nboto3  # for AWS Kinesis\\nkafka-python  # for Apache Kafka\\n```',\n",
       " 'description\\n\\nSure, here\\'s a possible starter code for these files:\\n\\n```\\n├── main.py\\n├── dashboard\\n│   ├── __init__.py\\n│   ├── app.py\\n│   ├── components.py\\n│   └── styles.css\\n├── stream_processor\\n│   ├── __init__.py\\n│   ├── config.py\\n│   └── consumer.py\\n└── requirements.txt\\n```\\n\\n```python\\n# main.py\\n\\nfrom dashboard.app import run_dashboard\\nfrom stream_processor.consumer import run_consumer\\n\\n\\nif __name__ == \"__main__\":\\n    # Start the Stream Processor\\n    run_consumer()\\n\\n    # Start the Dashboard\\n    run_dashboard()\\n```\\n\\n```python\\n# dashboard/app.py\\n\\nimport streamlit as st\\n\\n\\ndef run_dashboard():\\n    st.title(\"Real-time Data Analytics Dashboard\")\\n    \\n    # Add your code here...\\n```\\n\\n```python\\n# dashboard/components.py\\n\\nimport streamlit as st\\nimport pandas as pd\\n\\n\\ndef render_dataframe(df: pd.DataFrame) -> None:\\n    st.table(df)\\n\\n\\ndef render_line_chart(df: pd.DataFrame, x: str, y: str) -> None:\\n    chart_data = df[[x, y]]\\n    st.line_chart(chart_data)\\n```\\n\\n```css\\n/* dashboard/styles.css */\\n\\nbody {\\n  margin: 0;\\n  padding: 0;\\n  font-family: sans-serif;\\n}\\n\\n.container {\\n  max-width: 1200px;\\n  margin: 0 auto;\\n  padding: 20px;\\n}\\n\\nh1 {\\n  text-align: center;\\n  margin: 20px 0;\\n}\\n\\ntable {\\n  border-collapse: collapse;\\n  border-spacing: 0;\\n  width: 100%;\\n  border: 1px solid #ddd;\\n}\\n\\ntable th,\\ntable td {\\n  text-align: left;\\n  padding: 8px;\\n}\\n\\ntable th {\\n  background-color: #f2f2f2;\\n  border-bottom: 1px solid #ddd;\\n}\\n\\ntable td {\\n  border-bottom: 1px solid #ddd;\\n}\\n\\n.chart {\\n  width: 100%;\\n  height: 300px;\\n}\\n```\\n\\n```python\\n# stream_processor/config.py\\n\\nKINESIS_CONFIG = {\\n    \"stream_name\": \"my-stream\",\\n    \"region_name\": \"us-west-2\",\\n}\\n```\\n\\n```python\\n# stream_processor/consumer.py\\n\\nfrom stream_processor.config import KINESIS_CONFIG\\n\\n\\ndef run_consumer():\\n    # Add your code here...\\n```\\n\\n```\\n# requirements.txt\\n\\nfastapi\\nstreamlit\\npandas\\nboto3  # for AWS Kinesis\\nkafka-python  # for Apache Kafka\\n```']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_with_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements:\n",
      "\n",
      "```\n",
      "realtime_data_processing_pipeline/\n",
      "├── app/\n",
      "│   ├── __init__.py\n",
      "│   ├── main.py\n",
      "│   └── models.py\n",
      "├── data/\n",
      "│   └── sample_data.json\n",
      "├── tests/\n",
      "│   ├── __init__.py\n",
      "│   └── test_main.py\n",
      "├── .env\n",
      "├── .gitignore\n",
      "├── Dockerfile\n",
      "├── README.md\n",
      "└── requirements.txt\n",
      "```\n",
      "\n",
      "In `app/__init__.py`:\n",
      "\n",
      "```python\n",
      "# empty file\n",
      "```\n",
      "\n",
      "In `app/models.py`:\n",
      "\n",
      "```python\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class DataModel(BaseModel):\n",
      "    data: dict\n",
      "```\n",
      "\n",
      "In `app/main.py`:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from fastapi.responses import HTMLResponse\n",
      "from typing import List\n",
      "import json\n",
      "\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# sample data\n",
      "DATA = []\n",
      "\n",
      "# routes\n",
      "@app.get(\"/\")\n",
      "async def root():\n",
      "    return {\"message\": \"Hello World\"}\n",
      "\n",
      "@app.get(\"/data\", response_class=HTMLResponse)\n",
      "async def view_data():\n",
      "    \"\"\"\n",
      "    View the stored data.\n",
      "    \"\"\"\n",
      "    html_str = f\"<html><body>{json.dumps(DATA)}</body></html>\"\n",
      "    return html_str\n",
      "\n",
      "# endpoints\n",
      "@app.post(\"/api/data/\")\n",
      "async def create_data(data: dict):\n",
      "    \"\"\"\n",
      "    Receive real-time data via HTTP POST requests.\n",
      "    \"\"\"\n",
      "    DATA.append(data)\n",
      "\n",
      "    return {\"message\": \"Data created\"}\n",
      "\n",
      "@app.get(\"/api/data/\")\n",
      "async def read_data() -> List[DataModel]:\n",
      "    \"\"\"\n",
      "    Read the stored data.\n",
      "    \"\"\"\n",
      "    data_list = [DataModel(data=d) for d in DATA]\n",
      "\n",
      "    return data_list\n",
      "```\n",
      "\n",
      "In `data/sample_data.json`:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"sample_data\": \"example\"\n",
      "}\n",
      "```\n",
      "\n",
      "In `tests/__init__.py`:\n",
      "\n",
      "```python\n",
      "# empty file\n",
      "```\n",
      "\n",
      "In `tests/test_main.py`:\n",
      "\n",
      "```python\n",
      "from fastapi.testclient import TestClient\n",
      "from ..app.main import app\n",
      "\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "\n",
      "def test_read_data():\n",
      "    response = client.get(\"/api/data/\")\n",
      "    assert response.status_code == 200\n",
      "\n",
      "def test_create_data():\n",
      "    data = {\"data\": {\"test_data\": \"example\"}}\n",
      "    response = client.post(\"/api/data/\", json=data)\n",
      "    assert response.status_code == 200\n",
      "    assert response.json() == {\"message\": \"Data created\"}\n",
      "```\n",
      "\n",
      "In `.env`:\n",
      "\n",
      "```\n",
      "# AWS credentials\n",
      "AWS_ACCESS_KEY_ID=<your_access_key_id>\n",
      "AWS_SECRET_ACCESS_KEY=<your_secret_access_key>\n",
      "```\n",
      "\n",
      "In `.gitignore`:\n",
      "\n",
      "```\n",
      "# Byte-compiled / optimized / DLL files\n",
      "__pycache__/\n",
      "*.py[cod]\n",
      "*$py.class\n",
      "\n",
      "# C extensions\n",
      "*.so\n",
      "\n",
      "# Distribution / packaging\n",
      "/dist/\n",
      "/build/\n",
      "*.egg-info/\n",
      "\n",
      "# IDE helper files\n",
      "/.idea\n",
      "*.iml\n",
      "*.ipr\n",
      "*.iws\n",
      "\n",
      "# Compiled Python files\n",
      "*.pyc\n",
      "\n",
      "# Log files\n",
      "*.log\n",
      "\n",
      "# Custom extension for your project\n",
      ".env\n",
      "```\n",
      "\n",
      "In `Dockerfile`:\n",
      "\n",
      "```\n",
      "FROM python:3.8-slim-buster\n",
      "\n",
      "WORKDIR /code\n",
      "\n",
      "COPY requirements.txt .\n",
      "\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "COPY app/ app/\n",
      "COPY data/ data/\n",
      "COPY tests/ tests/\n",
      "\n",
      "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n",
      "```\n",
      "\n",
      "In `README.md`:\n",
      "\n",
      "```\n",
      "# Real-time Data Processing Pipeline\n",
      "\n",
      "This project provides a real-time data processing pipeline using FastAPI, Streamlit, AWS Kinesis, and AWS S3. Data is received via RESTful API endpoints, visualized using Streamlit, and processed and stored in real-time using AWS Kinesis and S3.\n",
      "\n",
      "## Requirements\n",
      "\n",
      "- Python 3.8 or later\n",
      "- Docker 20.10 or later\n",
      "- AWS account credentials\n",
      "- Streamlit\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "1. Clone this repository using `git clone https://github.com/<your-username>/realtime_data_processing_pipeline.git`\n",
      "2. Create an environment file from `.env.sample` and fill in the credentials for AWS Kinesis and S3.\n",
      "3. Run `docker build -t realtime-data-processing-pipeline .` to create a Docker image of the application.\n",
      "4. Run `docker run --env-file .env -p 80:80 realtime-data-processing-pipeline` to start the application.\n",
      "5. Navigate to `http://localhost` in your browser to view the Streamlit dashboard and interact with the application.\n",
      "\n",
      "## License\n",
      "\n",
      "This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(context[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
